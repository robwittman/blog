---
title: Ideating our models
---

The general idea of Orthus, is that software engineers can plug their app into Orthus through an API. On user / organization / team 
creation, hooks are sent to Orthus, which provisions a new "tenant" object. Operating on that tenant object, Orthus will provision 
the account / project / etc for that entity (or use a provided one)

## Terminology
- `Entity`: This represents the identity of a resource from the calling application. Can be a user, organization, or any related resource
- `Tenant`: The object inside Orthus, that represents a deployed environment 
- `Credentials`: The credentials for a given cloud account. These can be generated by Orthus, or if desired, provided by the caller 
- `Account`: Abstract representation of an AWS Account, GCP project, Azure Susbscription, or any further supported cloud providers
- `Leader`: The crossplane deployed alonside Orthus, for managing tenant installations
- `Director`: The crossplane instance deployed in a tenant cluster
- `Package`: An OCI artifact that represents a deployment package of "vendored" software
- `PackageRevision`: The versioning for `Package`s deployed to a `Tenant`. Not used currently, but we will eventually

## Tenant Deployment 
When Orthus receives a Create / Update event for a given `Entity`, we want to bring that `Tenant` to desired state

- If no `Account` and one not provided by caller, generate the requisite resource. Note that this requires high permissions for Orthus 
- If `Account` provided, simply take the provided credentials
- Store these `Credentials` objects in Vault (to be deployed later)
- Create an IAM role for `Leader` to manage the account

Orthus should now have an `Account` for the `Tenant`, as well as a set of `Credentials` that can be used. To start provisioning (AWS used as example):
- Create the VPC and any account wide configurations (cloudtrail, cloudwatch, etc)
- Create the IAM role for `Director`
- Create the EKS cluster, with any attached addons
- Install `Director` in the remote cluster, with the credentials generated above 
- Install any necessary providers alongside `Director`
- Lastly, install the `Package` to deploy the vendored software

At the moment, we're going to push any changes to the OCI directly to all connected tenants. Eventually, this ought to 
respect `PackageRevision` instead, so that we can push updates to environments individually, to allow for different 
cadences or SLAs

## Thoughts 
I _really_ want to use [Cluster API] for the deployment of the managed Kubernetes clusters. If we do, it will be done through Crossplane 
rather than creating the manifests separately. I need to weight the pros and cons, as it brings in more abstraction, as well as needing 
to configure provider resources for both Crossplane and ClusterAPI. But, its super neat, so there's that.

## Users / Access
Looking at the access levels of Orthus, we want a few different modules to differing clients
- `Api`: Used by the calling application to perform CRUD operations on objects inside Orthus
- `Webhooks`: Alternative method for clients to trigger tenant operations. For example, folks might want to provision something anytime someone signs up in Auth0. This prevents writing glue code just to transmit those events to Orthus
- `Admin`: For super users, either direct username / password login, or an Identity Provider like Okta / Auth0. For folks with administrator access that need to inspect or troubleshoot resources and operations
- `Tenant`: Access to a specific tenant (maybe white labeling?) for clients on that tenant to view status, request support, push updates, etc 

Note on above: At the moment, I don't have plans for tenants to have an individual database. They'll be namespaced in various tables by the `tenant_id`. 

## Background Job Processing
Catalog jobs in Tekton? Generic jobs in Sidekiq? Not sure yet
